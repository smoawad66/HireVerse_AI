{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI_g-rqk6vT5",
        "outputId": "11293714-b12a-49b7-ab20-4f521a1276bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: moviepy in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (2.1.2)\n",
            "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (5.1.1)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: numpy>=1.25.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: python-dotenv>=0.10 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (1.1.0)\n",
            "Requirement already satisfied: pillow<11.0,>=9.2.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from moviepy) (10.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from proglog<=1.0.0->moviepy) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from tqdm->proglog<=1.0.0->moviepy) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEqdAIfI93Kj",
        "outputId": "17e0620c-c688-4c76-e2c2-033e79fcfab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: audio-extract in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from audio-extract) (0.2.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.8 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from audio-extract) (0.4.8)\n",
            "Requirement already satisfied: mutagen==1.46.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from audio-extract) (1.46.0)\n",
            "Requirement already satisfied: future in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from ffmpeg-python==0.2.0->audio-extract) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install audio-extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXod09aj-C4L",
        "outputId": "b300d707-7dcc-426a-ea11-34578844ba5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/riad-azz/audio-extract.git\n",
            "  Cloning https://github.com/riad-azz/audio-extract.git to c:\\users\\compu magic\\appdata\\local\\temp\\pip-req-build-73e490j_\n",
            "  Resolved https://github.com/riad-azz/audio-extract.git to commit caf33e33418905acefbd287ced128a5e667bb3c7\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from audio_extract==0.7.0) (0.2.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.8 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from audio_extract==0.7.0) (0.4.8)\n",
            "Requirement already satisfied: mutagen==1.46.0 in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from audio_extract==0.7.0) (1.46.0)\n",
            "Requirement already satisfied: future in c:\\users\\compu magic\\anaconda3\\envs\\new_env\\lib\\site-packages (from ffmpeg-python==0.2.0->audio_extract==0.7.0) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/riad-azz/audio-extract.git 'C:\\Users\\compu magic\\AppData\\Local\\Temp\\pip-req-build-73e490j_'\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/riad-azz/audio-extract.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpaIOcGe-LR2",
        "outputId": "a63714e7-7ae4-4723-9c85-0f6a46107462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to extract audio and save to audio.mp3\n",
            "Success : audio file has been saved to \"c:\\Users\\compu magic\\Downloads\\tech\\audio.mp3\".\n",
            "Successfully created MP3 file at 'audio.mp3'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from audio_extract import extract_audio\n",
        "\n",
        "# Define the intended output path with the .mp3 extension\n",
        "intended_output_path = \"audio.mp3\"\n",
        "# Define the path that the function is actually creating\n",
        "actual_output_path = intended_output_path  # No need for additional extension since we want mp3\n",
        "\n",
        "print(f\"Attempting to extract audio and save to {intended_output_path}\")\n",
        "\n",
        "# Call the function to extract audio and save as MP3\n",
        "extract_audio(input_path=\"video.mp4\",\n",
        "              output_path=intended_output_path,\n",
        "              overwrite=True)\n",
        "\n",
        "# Verify the file was created\n",
        "if os.path.exists(intended_output_path):\n",
        "    print(f\"Successfully created MP3 file at '{intended_output_path}'\")\n",
        "else:\n",
        "    print(f\"File '{intended_output_path}' was not created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os # Import os to check if the file exists\n",
        "\n",
        "url = \"https://advanced-speech-to-text-fast-accurate-and-ai-powered.p.rapidapi.com/transcribe\"\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# Ensure 'downloaded_audio.mp3' exists in the same directory as your script\n",
        "# or provide the full path to the file.\n",
        "file_path = \"audio.mp3\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: Audio file not found at {file_path}\")\n",
        "    # You might want to exit or handle this error appropriately\n",
        "else:\n",
        "    try:\n",
        "        # Open the audio file in binary read mode\n",
        "        # Use 'with' to ensure the file is closed automatically\n",
        "        with open(file_path, 'rb') as audio_file:\n",
        "            # Prepare the files dictionary for requests\n",
        "            # The key 'audio_file' matches the expected form field name\n",
        "            # The value is a tuple: (filename, file_object)\n",
        "            files = {\n",
        "                'audio_file': (os.path.basename(file_path), audio_file)\n",
        "            }\n",
        "\n",
        "            # Define headers (excluding the manual Content-Type)\n",
        "            headers = {\n",
        "                \"x-rapidapi-key\": \"a7706886c9mshca04b4e277c7245p1ba4b3jsn8a50361f0d73\",\n",
        "                \"x-rapidapi-host\": \"advanced-speech-to-text-fast-accurate-and-ai-powered.p.rapidapi.com\",\n",
        "                # requests will automatically add the correct Content-Type header\n",
        "                # with the boundary when you use the 'files' parameter.\n",
        "                # Do NOT set Content-Type manually here.\n",
        "            }\n",
        "\n",
        "            # Make the POST request using the 'files' parameter\n",
        "            response = requests.post(url, headers=headers, files=files)\n",
        "\n",
        "            # Check if the request was successful before trying to parse JSON\n",
        "            if response.status_code == 200:\n",
        "                print(\"Request successful!\")\n",
        "                try:\n",
        "                    json_response = response.json()\n",
        "                    \n",
        "                  \n",
        "\n",
        "                    transcribed_text = None \n",
        "\n",
        "                    # --- THIS SECTION MUST BE ADJUSTED BASED ON THE ACTUAL RESPONSE ABOVE ---\n",
        "                    # Look at the 'Full JSON Response' output from THIS API (not the previous one).\n",
        "                    # Common keys: 'text', 'transcription', 'result', 'transcript', etc.\n",
        "\n",
        "                    # EXAMPLE A: If the text is directly under a key like 'text'\n",
        "                    if 'text' in json_response:\n",
        "                        transcribed_text = json_response['text']\n",
        "                    \n",
        "                    # EXAMPLE B: If the text is under a different common key like 'transcription'\n",
        "                    elif 'transcription' in json_response:\n",
        "                        transcribed_text = json_response['transcription']\n",
        "                    \n",
        "                    # EXAMPLE C: If the text is nested (e.g., within a 'data' object)\n",
        "                    elif 'data' in json_response and 'text' in json_response['data']:\n",
        "                        transcribed_text = json_response['data']['text']\n",
        "                    \n",
        "            \n",
        "\n",
        "                    # --- Store the extracted text ---\n",
        "                    if transcribed_text is not None:\n",
        "                        print(\"Extracted Transcribed Text:\")\n",
        "                        print(transcribed_text)\n",
        "\n",
        "                        output_filename = \"transcribed_audio_advanced_api.txt\" # Using a distinct filename\n",
        "                        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                            f.write(transcribed_text)\n",
        "                        print(f\"\\nTranscribed text saved to {output_filename}\")\n",
        "                    else:\n",
        "                        print(\"\\nCould not find transcription text in the response.\")\n",
        "                        print(\"Please examine the 'Full JSON Response' above and adjust the code to access the correct key for THIS API.\")\n",
        "\n",
        "                except requests.exceptions.JSONDecodeError:\n",
        "                    print(\"Error: Could not decode JSON response. Is the response valid JSON?\")\n",
        "                    print(\"Response body:\", response.text)\n",
        "            else:\n",
        "                print(f\"Request failed with status code {response.status_code}\")\n",
        "                print(\"Response body:\", response.text) # Print raw body for error details\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"A request error occurred: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import os\n",
        "\n",
        "# url = \"https://speech-to-text25.p.rapidapi.com/transcribe\"\n",
        "\n",
        "# file_path = \"downloaded_audio.mp3\" # Make sure this file exists\n",
        "\n",
        "# headers = {\n",
        "#     \"x-rapidapi-key\": \"a7706886c9mshca04b4e277c7245p1ba4b3jsn8a50361f0d73\", # Use YOUR actual key\n",
        "#     \"x-rapidapi-host\": \"speech-to-text25.p.rapidapi.com\",\n",
        "# }\n",
        "\n",
        "# data = {} # Add other form fields here if required by API (e.g., language)\n",
        "\n",
        "# if not os.path.exists(file_path):\n",
        "#     print(f\"Error: Audio file not found at {file_path}\")\n",
        "# else:\n",
        "#     try:\n",
        "#         with open(file_path, 'rb') as audio_file:\n",
        "#             files = {\n",
        "#                 'audio_file': (os.path.basename(file_path), audio_file)\n",
        "#             }\n",
        "\n",
        "#             response = requests.post(url, headers=headers, files=files, data=data)\n",
        "\n",
        "#             if response.status_code == 200:\n",
        "#                 print(\"Request successful!\")\n",
        "#                 try:\n",
        "#                     json_response = response.json()\n",
        "                    \n",
        "                  \n",
        "\n",
        "#                     transcribed_text = None \n",
        "\n",
        "#                     # --- THIS SECTION WILL BE ADJUSTED AFTER WE SEE THE ACTUAL RESPONSE ---\n",
        "#                     # Example of how it *might* be (you'll update this based on the actual output)\n",
        "#                     # IF the text is directly under a key like 'text':\n",
        "#                     if 'text' in json_response:\n",
        "#                         transcribed_text = json_response['text']\n",
        "#                     # IF the text is under a 'result' key:\n",
        "#                     elif 'result' in json_response and 'text' in json_response['result']:\n",
        "#                         transcribed_text = json_response['result']['text']\n",
        "#                     # ELSE IF your original example was correct but only for a specific scenario:\n",
        "#                     # elif 'transcription' in json_response and 'full_transcript' in json_response['transcription']:\n",
        "#                     #    transcribed_text = json_response['transcription']['full_transcript']\n",
        "#                     # ... and so on.\n",
        "\n",
        "#                     # --- Store the extracted text ---\n",
        "#                     if transcribed_text is not None:\n",
        "#                         print(\"Extracted Transcribed Text:\")\n",
        "#                         print(transcribed_text)\n",
        "\n",
        "#                         output_filename = \"transcribed_audio.txt\"\n",
        "#                         with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "#                             f.write(transcribed_text)\n",
        "#                         print(f\"\\nTranscribed text saved to {output_filename}\")\n",
        "#                     else:\n",
        "#                         print(\"\\nCould not find transcription text in the response.\")\n",
        "#                         print(\"Please examine the 'Full JSON Response' above and adjust the code to access the correct key.\")\n",
        "\n",
        "#                 except requests.exceptions.JSONDecodeError:\n",
        "#                     print(\"Error: Could not decode JSON response. Is the response valid JSON?\")\n",
        "#                     print(\"Response body:\", response.text)\n",
        "#             else:\n",
        "#                 print(f\"Request failed with status code {response.status_code}\")\n",
        "#                 print(\"Response body:\", response.text) # Print raw body for error details\n",
        "\n",
        "#     except requests.exceptions.RequestException as e:\n",
        "#         print(f\"A request error occurred: {e}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-08 16:16:38,133 - INFO - Downloading NLTK wordnet...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Modified Technical Interview Answer Evaluator ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-08 16:16:38,295 - INFO - Loaded 10 questions from mid_level_software_engineer_data_scientist_questions.json\n",
            "2025-06-08 16:16:38,304 - INFO - Evaluating answer 1/2...\n",
            "2025-06-08 16:16:38,495 - INFO - Loading cached model from bert_model.pkl\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluating 2 Sample Answers ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-08 16:17:21,576 - INFO - Model loaded from cache successfully.\n",
            "2025-06-08 16:17:38,118 - INFO - Evaluating answer 2/2...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluation 1 ---\n",
            "Question: What is the difference between the 'is' and '==' operators in Python?\n",
            "Answer: The 'is' operator is for identity, it checks if two variables point to the same object in memory. In contrast, the '==' operator is for equality, meaning it checks if two objects have the same value. For instance, two separate lists with identical contents are equal (== is True) but are not the same object (is is False).\n",
            "Score: 0.877 (Excellent)\n",
            "Components: K=1.00, C=0.95, Crit=0.69\n",
            "Matched Keywords: ['identity', 'operator', 'equality']\n",
            "\n",
            "--- Evaluation 2 ---\n",
            "Question: How does Python handle memory management, and what is the role of the garbage collector?\n",
            "Answer: Python's memory management is done on a private heap. It uses reference counting to track object references. When an object's reference count is zero, it gets deallocated. There's also a garbage collector that handles cyclic references which reference counting alone cannot solve.\n",
            "Score: 0.892 (Good)\n",
            "Components: K=1.00, C=0.89, Crit=0.79\n",
            "Matched Keywords: ['garbage collector', 'reference counting', 'memory management']\n",
            "\n",
            "\n",
            "âœ… Evaluation results saved to evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Set, Optional, Union\n",
        "from functools import lru_cache\n",
        "import logging\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- NLTK Lazy Loading and Setup ---\n",
        "_nltk_resources = {}\n",
        "\n",
        "def lazy_load_nltk_resources():\n",
        "    \"\"\"\n",
        "    Lazy loads and sets up NLTK resources. This function is called only once\n",
        "    when TextProcessor is initialized.\n",
        "    \"\"\"\n",
        "    if _nltk_resources:  # Already loaded\n",
        "        return\n",
        "    try:\n",
        "        import nltk\n",
        "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "        from nltk.corpus import stopwords\n",
        "        required_data = [\n",
        "            ('punkt', 'tokenizers/punkt'),\n",
        "            ('wordnet', 'corpora/wordnet'),\n",
        "            ('stopwords', 'corpora/stopwords'),\n",
        "        ]\n",
        "        for package, path in required_data:\n",
        "            try:\n",
        "                nltk.data.find(path)\n",
        "            except LookupError:\n",
        "                logging.info(f\"Downloading NLTK {package}...\")\n",
        "                nltk.download(package, quiet=True)\n",
        "        _nltk_resources['word_tokenize'] = word_tokenize\n",
        "        _nltk_resources['sent_tokenize'] = sent_tokenize\n",
        "        _nltk_resources['lemmatizer'] = WordNetLemmatizer()\n",
        "        _nltk_resources['stop_words'] = set(stopwords.words('english'))\n",
        "    except ImportError:\n",
        "        logging.error(\"NLTK is required. Install with: pip install nltk\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error initializing NLTK resources: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Data Classes ---\n",
        "@dataclass(frozen=True)\n",
        "class ThresholdConfig:\n",
        "    \"\"\"Configuration for score thresholds (excellent, good, fair).\"\"\"\n",
        "    excellent: float\n",
        "    good: float\n",
        "    fair: float\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class WeightProfile:\n",
        "    \"\"\"Weights for different scoring components.\"\"\"\n",
        "    keyword: float\n",
        "    component: float\n",
        "    criteria: float\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate that weights sum approximately to 1.0.\"\"\"\n",
        "        total = self.keyword + self.component + self.criteria\n",
        "        if not (0.95 <= total <= 1.05):  # Allow a small tolerance for floating point arithmetic\n",
        "            raise ValueError(f\"Weights must sum to ~1.0, got {total}\")\n",
        "\n",
        "@dataclass\n",
        "class EvaluationConfig:\n",
        "    \"\"\"Global configuration for the answer evaluation system.\"\"\"\n",
        "    difficulty_thresholds: Dict[str, ThresholdConfig] = field(default_factory=lambda: {\n",
        "        \"Easy\": ThresholdConfig(excellent=0.8, good=0.65, fair=0.45),\n",
        "        \"Medium\": ThresholdConfig(excellent=0.85, good=0.7, fair=0.5),\n",
        "        \"Hard\": ThresholdConfig(excellent=0.9, good=0.75, fair=0.55)\n",
        "    })\n",
        "    default_weights: WeightProfile = WeightProfile(keyword=1/3, component=1/3, criteria=1/3)\n",
        "    experience_modifiers: Dict[str, float] = field(default_factory=lambda: {\n",
        "        \"Junior\": 0.9, \"Mid-level\": 1.0, \"Senior\": 1.1\n",
        "    })\n",
        "    similarity_threshold: float = 0.6\n",
        "    min_answer_length: int = 10  # Minimum number of words for an answer to be considered valid\n",
        "    model_name: str = 'all-MiniLM-L6-v2'\n",
        "    model_cache_path: str = 'bert_model.pkl'\n",
        "\n",
        "\n",
        "\n",
        "# --- Text Processing ---\n",
        "class TextProcessor:\n",
        "    \"\"\"Handles text preprocessing tasks like tokenization, lemmatization, and stop word removal.\"\"\"\n",
        "    def __init__(self):\n",
        "        lazy_load_nltk_resources()  # Ensure NLTK resources are loaded\n",
        "        self._word_tokenize = _nltk_resources['word_tokenize']\n",
        "        self._sent_tokenize = _nltk_resources['sent_tokenize']\n",
        "        self._lemmatizer = _nltk_resources['lemmatizer']\n",
        "        self._stop_words = _nltk_resources['stop_words']\n",
        "        self._whitespace_pattern = re.compile(r'\\s+')\n",
        "        self._punctuation_translation_table = str.maketrans('', '', string.punctuation.replace('-', ''))\n",
        "\n",
        "    @lru_cache(maxsize=1000)\n",
        "    def preprocess_text(self, text: str) -> Tuple[str, ...]:\n",
        "        \"\"\"\n",
        "        Tokenizes, normalizes, removes punctuation/stopwords, and lemmatizes text.\n",
        "        \"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return tuple()\n",
        "        try:\n",
        "            normalized = self._whitespace_pattern.sub(' ', text).lower().strip()\n",
        "            normalized = normalized.translate(self._punctuation_translation_table)\n",
        "            tokens = self._word_tokenize(normalized)\n",
        "            processed_tokens = [\n",
        "                self._lemmatizer.lemmatize(word)\n",
        "                for word in tokens\n",
        "                if word.isalpha() and len(word) > 1 and word not in self._stop_words\n",
        "            ]\n",
        "            return tuple(processed_tokens)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Text preprocessing error for text: '{text[:50]}...'. Error: {e}\", exc_info=True)\n",
        "            return tuple(text.lower().split())\n",
        "\n",
        "    def tokenize_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenizes text into a list of sentences.\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return []\n",
        "        try:\n",
        "            return self._sent_tokenize(text)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Sentence tokenization error for text: '{text[:50]}...'. Error: {e}\", exc_info=True)\n",
        "            return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "\n",
        "# --- Model Management ---\n",
        "class ModelManager:\n",
        "    \"\"\"Manages loading and caching of SentenceTransformer models.\"\"\"\n",
        "    def __init__(self, model_name: str, cache_path: str):\n",
        "        self.model_name = model_name\n",
        "        self.cache_path = Path(cache_path)\n",
        "        self._model: Optional[SentenceTransformer] = None\n",
        "\n",
        "    @property\n",
        "    def model(self) -> SentenceTransformer:\n",
        "        \"\"\"Lazily loads the SentenceTransformer model.\"\"\"\n",
        "        if self._model is None:\n",
        "            self._model = self._load_model()\n",
        "        return self._model\n",
        "\n",
        "    def _load_model(self) -> SentenceTransformer:\n",
        "        \"\"\"Loads the SentenceTransformer model from cache or downloads it.\"\"\"\n",
        "        if self.cache_path.exists():\n",
        "            try:\n",
        "                logging.info(f\"Loading cached model from {self.cache_path}\")\n",
        "                with open(self.cache_path, 'rb') as f:\n",
        "                    model = pickle.load(f)\n",
        "                logging.info(\"Model loaded from cache successfully.\")\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Failed to load cached model: {e}. Downloading new model.\", exc_info=True)\n",
        "        logging.info(f\"Downloading model: {self.model_name}\")\n",
        "        model = SentenceTransformer(self.model_name)\n",
        "        try:\n",
        "            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with open(self.cache_path, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "            logging.info(f\"Model cached to {self.cache_path}\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to cache model: {e}\", exc_info=True)\n",
        "        return model\n",
        "\n",
        "# --- Scoring Engine ---\n",
        "class ScoringEngine:\n",
        "    \"\"\"Calculates various scores for an applicant's answer against expected criteria.\"\"\"\n",
        "    def __init__(self, text_processor: TextProcessor, model_manager: ModelManager, config: EvaluationConfig):\n",
        "        self.text_processor = text_processor\n",
        "        self.model_manager = model_manager\n",
        "        self.config = config\n",
        "\n",
        "    def calculate_keyword_score(self, applicant_tokens: Tuple[str, ...],\n",
        "                                expected_keywords: List[str]) -> Tuple[List[str], float]:\n",
        "        \"\"\"Calculates a keyword match score.\"\"\"\n",
        "        if not expected_keywords:\n",
        "            return [], 1.0\n",
        "        if not applicant_tokens:\n",
        "            return [], 0.0\n",
        "        processed_expected_keywords = {\n",
        "            self.text_processor._lemmatizer.lemmatize(kw.lower())\n",
        "            for kw in expected_keywords\n",
        "        }\n",
        "        matched_keywords_found = set()\n",
        "        applicant_token_set = set(applicant_tokens)\n",
        "        direct_matches = applicant_token_set.intersection(processed_expected_keywords)\n",
        "        matched_keywords_found.update(direct_matches)\n",
        "        for expected_kw in processed_expected_keywords:\n",
        "            if expected_kw in applicant_token_set:\n",
        "                matched_keywords_found.add(expected_kw)\n",
        "                continue\n",
        "            if any(expected_kw in token or token in expected_kw for token in applicant_token_set):\n",
        "                matched_keywords_found.add(expected_kw)\n",
        "        score = len(matched_keywords_found) / len(processed_expected_keywords)\n",
        "        return list(matched_keywords_found), min(score, 1.0)\n",
        "\n",
        "    def calculate_component_coverage(self, applicant_answer: str,\n",
        "                                     expected_components: List[str]) -> Tuple[List[str], float]:\n",
        "        \"\"\"Calculates coverage of expected components using sentence embeddings.\"\"\"\n",
        "        if not expected_components:\n",
        "            return [], 1.0\n",
        "        applicant_sentences = self.text_processor.tokenize_sentences(applicant_answer)\n",
        "        if not applicant_sentences:\n",
        "            return [], 0.0\n",
        "        try:\n",
        "            all_texts = applicant_sentences + expected_components\n",
        "            embeddings = self.model_manager.model.encode(all_texts, batch_size=32, show_progress_bar=False)\n",
        "            n_sentences = len(applicant_sentences)\n",
        "            sentence_embeddings = embeddings[:n_sentences]\n",
        "            component_embeddings = embeddings[n_sentences:]\n",
        "            similarities = cosine_similarity(sentence_embeddings.astype(np.float32), component_embeddings.astype(np.float32))\n",
        "            component_max_similarities = np.max(similarities, axis=0)\n",
        "            covered_components = [\n",
        "                comp for comp, score in zip(expected_components, component_max_similarities)\n",
        "                if score >= self.config.similarity_threshold\n",
        "            ]\n",
        "            overall_score = np.mean(component_max_similarities) if component_max_similarities.size > 0 else 0.0\n",
        "            return covered_components, float(overall_score)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Component coverage calculation error: {e}\", exc_info=True)\n",
        "            return [], 0.0\n",
        "\n",
        "    def calculate_criteria_score(self, applicant_answer: str, assessment_criteria: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculates a score based on how well the answer meets a specific assessment criterion\n",
        "        using sentence embeddings and cosine similarity.\n",
        "        \"\"\"\n",
        "        if not assessment_criteria or not applicant_answer.strip():\n",
        "            return 0.0\n",
        "        try:\n",
        "            # Encode the applicant's answer and the assessment criterion\n",
        "            embeddings = self.model_manager.model.encode(\n",
        "                [applicant_answer, assessment_criteria],\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "            answer_embedding = embeddings[0].reshape(1, -1)\n",
        "            criteria_embedding = embeddings[1].reshape(1, -1)\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity(answer_embedding, criteria_embedding)[0][0]\n",
        "            return float(np.clip(similarity, 0.0, 1.0))  # Clip score between 0 and 1\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Criteria score calculation error: {e}\", exc_info=True)\n",
        "            return 0.0\n",
        "\n",
        "# --- Answer Evaluator ---\n",
        "class OptimizedAnswerEvaluator:\n",
        "    \"\"\"Main class for evaluating technical interview answers.\"\"\"\n",
        "    def __init__(self, config: Optional[EvaluationConfig] = None):\n",
        "        self.config = config or EvaluationConfig()\n",
        "        self.text_processor = TextProcessor()\n",
        "        self.model_manager = ModelManager(self.config.model_name, self.config.model_cache_path)\n",
        "        self.scoring_engine = ScoringEngine(self.text_processor, self.model_manager, self.config)\n",
        "\n",
        "    def _get_weight_profile(self, question_data: Dict) -> WeightProfile:\n",
        "        \"\"\"Returns the weight profile for scoring.\"\"\"\n",
        "        return self.config.default_weights\n",
        "\n",
        "    def _get_adaptive_thresholds(self, question_data: Dict) -> ThresholdConfig:\n",
        "        \"\"\"Calculates adaptive scoring thresholds.\"\"\"\n",
        "        difficulty = question_data.get(\"Difficulty Level\", \"Medium\")\n",
        "        experience = question_data.get(\"Experience Level\", \"Mid-level\")\n",
        "        base_thresholds = self.config.difficulty_thresholds.get(difficulty, self.config.difficulty_thresholds[\"Medium\"])\n",
        "        modifier = self.config.experience_modifiers.get(experience, 1.0)\n",
        "        return ThresholdConfig(\n",
        "            excellent=float(np.clip(base_thresholds.excellent * modifier, 0.3, 0.95)),\n",
        "            good=float(np.clip(base_thresholds.good * modifier, 0.3, 0.95)),\n",
        "            fair=float(np.clip(base_thresholds.fair * modifier, 0.3, 0.95))\n",
        "        )\n",
        "\n",
        "    def _calculate_weighted_score(self, scores: Dict[str, float], weights: WeightProfile) -> float:\n",
        "        \"\"\"Calculates the overall weighted score.\"\"\"\n",
        "        return float(scores.get(\"keyword\", 0.0) * weights.keyword +\n",
        "                     scores.get(\"component\", 0.0) * weights.component +\n",
        "                     scores.get(\"criteria\", 0.0) * weights.criteria)\n",
        "\n",
        "    def _get_qualitative_assessment(self, score: float, thresholds: ThresholdConfig) -> str:\n",
        "        \"\"\"Determines the qualitative assessment (Excellent, Good, Fair, Poor).\"\"\"\n",
        "        if score >= thresholds.excellent:\n",
        "            return \"Excellent\"\n",
        "        elif score >= thresholds.good:\n",
        "            return \"Good\"\n",
        "        elif score >= thresholds.fair:\n",
        "            return \"Fair\"\n",
        "        else:\n",
        "            return \"Poor\"\n",
        "\n",
        "    def evaluate_answer(self, question_data: Dict, applicant_answer: str) -> Dict:\n",
        "        \"\"\"Evaluates a single applicant answer against question criteria.\"\"\"\n",
        "        try:\n",
        "            if not applicant_answer or len(applicant_answer.split()) < self.config.min_answer_length:\n",
        "                return self._create_error_result(\n",
        "                    question_data, applicant_answer,\n",
        "                    f\"Answer is too short or empty (min {self.config.min_answer_length} words required).\"\n",
        "                )\n",
        "            applicant_tokens = self.text_processor.preprocess_text(applicant_answer)\n",
        "            matched_kw, keyword_score = self.scoring_engine.calculate_keyword_score(\n",
        "                applicant_tokens, question_data.get(\"Expected Keywords\", [])\n",
        "            )\n",
        "            covered_comp, component_score = self.scoring_engine.calculate_component_coverage(\n",
        "                applicant_answer, question_data.get(\"Expected Answer Components\", [])\n",
        "            )\n",
        "            # Calculate the new criteria score\n",
        "            criteria_score = self.scoring_engine.calculate_criteria_score(\n",
        "                applicant_answer, question_data.get(\"Assessment Criteria\", \"\")\n",
        "            )\n",
        "            scores = {\n",
        "                \"keyword\": float(keyword_score),\n",
        "                \"component\": float(component_score),\n",
        "                \"criteria\": float(criteria_score)\n",
        "            }\n",
        "            weights = self._get_weight_profile(question_data)\n",
        "            overall_score = self._calculate_weighted_score(scores, weights)\n",
        "            scores[\"overall\"] = float(overall_score)\n",
        "            thresholds = self._get_adaptive_thresholds(question_data)\n",
        "            assessment = self._get_qualitative_assessment(overall_score, thresholds)\n",
        "            return {\n",
        "                \"question\": question_data.get(\"Question Text\", \"\"),\n",
        "                \"question_metadata\": {\n",
        "                    \"skill\": question_data.get(\"Skill\", \"\"),\n",
        "                    \"difficulty\": question_data.get(\"Difficulty Level\", \"\"),\n",
        "                    \"experience_level\": question_data.get(\"Experience Level\", \"\"),\n",
        "                    \"job_role\": question_data.get(\"Job Role\", \"\")\n",
        "                },\n",
        "                \"answer\": applicant_answer,\n",
        "                \"scores\": {k: round(v, 3) for k, v in scores.items()},\n",
        "                \"matched_keywords\": matched_kw,\n",
        "                \"assessment\": assessment,\n",
        "                \"thresholds_used\": {\n",
        "                    \"excellent\": float(thresholds.excellent),\n",
        "                    \"good\": float(thresholds.good),\n",
        "                    \"fair\": float(thresholds.fair)\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Evaluation error for question '{question_data.get('Question Text', '')}': {e}\", exc_info=True)\n",
        "            return self._create_error_result(question_data, applicant_answer, str(e))\n",
        "\n",
        "    def _create_error_result(self, question_data: Dict, answer: str, error: str) -> Dict:\n",
        "        \"\"\"Helper to create a consistent error result dictionary.\"\"\"\n",
        "        return {\n",
        "            \"question\": question_data.get(\"Question Text\", \"N/A\"),\n",
        "            \"answer\": answer,\n",
        "            \"error\": error,\n",
        "            \"scores\": {\"overall\": 0.0},\n",
        "            \"assessment\": \"Error\"\n",
        "        }\n",
        "\n",
        "    def evaluate_batch(self, questions: List[Dict], answers: List[str]) -> List[Dict]:\n",
        "        \"\"\"Evaluates a batch of answers.\"\"\"\n",
        "        if len(questions) != len(answers):\n",
        "            raise ValueError(\"Number of questions must match number of answers\")\n",
        "        results = []\n",
        "        for i, (question, answer) in enumerate(zip(questions, answers)):\n",
        "            logging.info(f\"Evaluating answer {i+1}/{len(questions)}...\")\n",
        "            results.append(self.evaluate_answer(question, answer))\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def load_questions(file_path: Union[str, Path]) -> List[Dict]:\n",
        "    \"\"\"Loads questions from a JSON file.\"\"\"\n",
        "    file_path = Path(file_path)\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            questions = json.load(f)\n",
        "        if not isinstance(questions, list):\n",
        "            raise ValueError(\"JSON file must contain a list of questions\")\n",
        "        logging.info(f\"Loaded {len(questions)} questions from {file_path}\")\n",
        "        return questions\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Questions file not found: {file_path}\")\n",
        "        return []\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.error(f\"Invalid JSON in {file_path}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading questions: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- JSON Custom Serializer ---\n",
        "def json_numpy_serializer(obj):\n",
        "    \"\"\"JSON serializer for numpy types.\"\"\"\n",
        "    if isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    print(\"=== Modified Technical Interview Answer Evaluator ===\\n\")\n",
        "    try:\n",
        "        evaluator = OptimizedAnswerEvaluator()\n",
        "        questions_file = 'mid_level_software_engineer_data_scientist_questions.json'\n",
        "        all_questions = load_questions(questions_file)\n",
        "        if not all_questions:\n",
        "            print(\"âŒ No questions loaded. Please create the JSON file with questions.\")\n",
        "            return\n",
        "        \n",
        "        questions_to_evaluate = all_questions[:2]\n",
        "        sample_answers = [\n",
        "            \"\"\"The 'is' operator is for identity, it checks if two variables point to the same object in memory. In contrast, the '==' operator is for equality, meaning it checks if two objects have the same value. For instance, two separate lists with identical contents are equal (== is True) but are not the same object (is is False).\"\"\",\n",
        "            \"\"\"Python's memory management is done on a private heap. It uses reference counting to track object references. When an object's reference count is zero, it gets deallocated. There's also a garbage collector that handles cyclic references which reference counting alone cannot solve.\"\"\"\n",
        "        ]\n",
        "        if len(questions_to_evaluate) > len(sample_answers):\n",
        "            print(f\"Warning: Only {len(sample_answers)} sample answers provided, but {len(questions_to_evaluate)} questions loaded for evaluation.\")\n",
        "            questions_to_evaluate = questions_to_evaluate[:len(sample_answers)]\n",
        "\n",
        "        print(f\"=== Evaluating {len(questions_to_evaluate)} Sample Answers ===\\n\")\n",
        "        batch_results = evaluator.evaluate_batch(questions_to_evaluate, sample_answers)\n",
        "        \n",
        "        for i, result in enumerate(batch_results):\n",
        "            print(f\"--- Evaluation {i+1} ---\")\n",
        "            if \"error\" in result:\n",
        "                print(f\"Question: {result.get('question', 'N/A')}\")\n",
        "                print(f\"Error: {result['error']}\\n\")\n",
        "                continue\n",
        "            print(f\"Question: {result['question']}\")\n",
        "            print(f\"Answer: {result['answer']}\")\n",
        "            print(f\"Score: {result['scores']['overall']:.3f} ({result['assessment']})\")\n",
        "            print(f\"Components: K={result['scores']['keyword']:.2f}, \"\n",
        "                  f\"C={result['scores']['component']:.2f}, \"\n",
        "                  f\"Crit={result['scores']['criteria']:.2f}\")\n",
        "            print(f\"Matched Keywords: {result['matched_keywords']}\\n\")\n",
        "        output_file_name = 'evaluation_results.json'\n",
        "        try:\n",
        "            with open(output_file_name, 'w', encoding='utf-8') as f:\n",
        "                json.dump(batch_results, f, indent=4, default=json_numpy_serializer)\n",
        "            print(f\"\\nâœ… Evaluation results saved to {output_file_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error saving evaluation results to JSON file: {e}\")\n",
        "            logging.error(f\"Error saving evaluation results: {e}\", exc_info=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error in main execution: {e}\")\n",
        "        logging.error(f\"Fatal error in main execution: {e}\", exc_info=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
